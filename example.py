from viztracer import VizTracer
import re
import time
import random
from collections import Counter

sample_text = """
Python is a versatile programming language that was created by Guido van Rossum
and first released in 1991. Python's design philosophy emphasizes code readability 
with its notable use of significant whitespace. Its language constructs as well as its
object-oriented approach aim to help programmers write clear, logical code for small and 
large-scale projects. Python is dynamically typed and garbage-collected. It supports 
multiple programming paradigms, including procedural, object-oriented, and functional 
programming. Python is often described as a "batteries included" language due to its 
comprehensive standard library.
"""

def get_timestamp():
    return time.strftime("%H:%M:%S.%f")[:-3]

def tokenize_text(text, min_length=3):
    tracer.log_instant(f"üö© –ü–†–û–§–ò–õ–¨: –ù–∞—á–∞–ª–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ [{get_timestamp()}]")
    
    tracer.log_instant("–ù–∞—á–∞–ª–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏")
    
    tracer.log_var("input_text_length", len(text))
    
    start_time = time.time()
    
    text = text.lower()
    
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    
    preprocess_time = time.time() - start_time
    tracer.log_instant(f"‚è±Ô∏è –ò–ó–ú–ï–†–ï–ù–ò–ï: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∑–∞–Ω—è–ª–∞ {preprocess_time:.4f}—Å")
    
    words = text.split()
    
    filtered_words = [word for word in words if len(word) >= min_length]
    
    tracer.log_var("words_before_filter", len(words))
    tracer.log_var("words_after_filter", len(filtered_words))
    
    tracer.log_instant("–ö–æ–Ω–µ—Ü —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏")
    
    total_time = time.time() - start_time
    tracer.log_instant(f"‚úÖ –ü–†–û–§–ò–õ–¨: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ [{get_timestamp()}] - {total_time:.4f}—Å")
    
    time.sleep(0.05)
    
    return filtered_words

def remove_stopwords(word_list, stopwords):
    tracer.log_instant(f"üö© –ü–†–û–§–ò–õ–¨: –ù–∞—á–∞–ª–æ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤ [{get_timestamp()}]")
    
    tracer.log_instant("–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤[")
    
    tracer.log_var("word_list_size", len(word_list))
    tracer.log_var("stopwords_count", len(stopwords))
    
    start_time = time.time()
    
    filtered_words = []
    removed_count = 0
    
    for i, word in enumerate(word_list):
        if i % 10 == 0:
            tracer.log_var("progress", i)
        
        if word not in stopwords:
            filtered_words.append(word)
        else:
            removed_count += 1
            
        if i == len(word_list) // 2:
            mid_time = time.time() - start_time
            tracer.log_instant(f"‚è±Ô∏è –ò–ó–ú–ï–†–ï–ù–ò–ï: –ü–æ–ª–æ–≤–∏–Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∑–∞ {mid_time:.4f}—Å")
    
    tracer.log_var("removed_count", removed_count)
    tracer.log_var("remaining_count", len(filtered_words))
    
    tracer.log_instant("–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤]")
    
    total_time = time.time() - start_time
    tracer.log_instant(f"‚úÖ –ü–†–û–§–ò–õ–¨: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ [{get_timestamp()}] - {total_time:.4f}—Å")
    
    time.sleep(0.05)
    
    return filtered_words

def calculate_word_frequency(word_list):
    tracer.log_instant(f"üö© –ü–†–û–§–ò–õ–¨: –ù–∞—á–∞–ª–æ —Ä–∞—Å—á–µ—Ç–∞ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ [{get_timestamp()}]")
    
    tracer.log_instant("–†–∞—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤[")
    
    tracer.log_var("unique_words", len(set(word_list)))
    tracer.log_var("total_words", len(word_list))
    
    start_time = time.time()
    
    if len(word_list) > 50:
        tracer.log_instant(f"‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ë–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä —Å–ª–æ–≤ ({len(word_list)}) –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è")
    
    time.sleep(0.1)
    
    word_freq = Counter(word_list)
    
    count_time = time.time() - start_time
    tracer.log_instant(f"‚è±Ô∏è –ò–ó–ú–ï–†–ï–ù–ò–ï: –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –∑–∞–Ω—è–ª {count_time:.4f}—Å")
    
    most_common = word_freq.most_common(5)
    
    most_common_str = ", ".join([f"{word}:{count}" for word, count in most_common[:3]])
    tracer.log_var("most_common_words", most_common_str)
    
    tracer.log_instant("–†–∞—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤]")
    
    total_time = time.time() - start_time
    tracer.log_instant(f"‚úÖ –ü–†–û–§–ò–õ–¨: –†–∞—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω [{get_timestamp()}] - {total_time:.4f}—Å")
    
    return word_freq

def process_text_segment(text_segment, segment_id):
    tracer.log_instant(f"üö© –ü–†–û–§–ò–õ–¨: –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞ {segment_id} [{get_timestamp()}]")
    
    tracer.log_instant(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ {segment_id}[")
    
    tracer.log_var(f"segment_{segment_id}_length", len(text_segment))
    
    start_time = time.time()
    
    tokens = tokenize_text(text_segment)
    
    tokenize_time = time.time() - start_time
    tracer.log_instant(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞ {segment_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    tracer.log_instant(f"‚è±Ô∏è –ò–ó–ú–ï–†–ï–ù–ò–ï: –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞ {segment_id} - {tokenize_time:.4f}—Å")
    
    stopwords = ['the', 'and', 'is', 'to', 'in', 'that', 'its', 'with', 'as', 'for']
    
    stopwords_start = time.time()
    filtered_tokens = remove_stopwords(tokens, stopwords)
    
    stopwords_time = time.time() - stopwords_start
    tracer.log_instant(f"–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ {segment_id} –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    tracer.log_instant(f"‚è±Ô∏è –ò–ó–ú–ï–†–ï–ù–ò–ï: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ —Å–µ–≥–º–µ–Ω—Ç–∞ {segment_id} - {stopwords_time:.4f}—Å")
    
    freq_start = time.time()
    word_freq = calculate_word_frequency(filtered_tokens)
    freq_time = time.time() - freq_start
    tracer.log_instant(f"‚è±Ô∏è –ò–ó–ú–ï–†–ï–ù–ò–ï: –†–∞—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–∞ {segment_id} - {freq_time:.4f}—Å")
    
    delay = random.uniform(0.05, 0.2)
    time.sleep(delay)
    
    result = {
        'segment_id': segment_id,
        'token_count': len(tokens),
        'filtered_count': len(filtered_tokens),
        'most_common': word_freq.most_common(3)
    }
    
    tracer.log_var(f"segment_{segment_id}_tokens", result['token_count'])
    tracer.log_var(f"segment_{segment_id}_filtered", result['filtered_count'])
    
    tracer.log_instant(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞ {segment_id}]")
    
    total_time = time.time() - start_time
    tracer.log_instant(f"‚úÖ –ü–†–û–§–ò–õ–¨: –°–µ–≥–º–µ–Ω—Ç {segment_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω [{get_timestamp()}] - {total_time:.4f}—Å")
    
    return result

def analyze_text(text):
    tracer.log_instant(f"üö© –ü–†–û–§–ò–õ–¨: –ù–∞—á–∞–ª–æ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ [{get_timestamp()}]")
    
    tracer.log_instant("–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞[")
    
    tracer.log_var("text_size", len(text))
    
    start_time = time.time()
    
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    tracer.log_var("sentence_count", len(sentences))
    
    tracer.log_instant(f"‚ÑπÔ∏è –ò–ù–§–û–†–ú–ê–¶–ò–Ø: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    results = []
    
    for i, sentence in enumerate(sentences):
        tracer.log_instant(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è {i}")
        
        result = process_text_segment(sentence, i)
        results.append(result)
        
        tracer.log_instant(f"–ö–æ–Ω–µ—Ü –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è {i}")
        
        progress_pct = (i + 1) / len(sentences) * 100
        tracer.log_instant(f"üìä –ü–†–û–ì–†–ï–°–°: –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i+1}/{len(sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ({progress_pct:.1f}%)")
    
    total_tokens = sum(r['token_count'] for r in results)
    total_filtered = sum(r['filtered_count'] for r in results)
    
    process_time = time.time() - start_time
    tracer.log_instant(f"‚è±Ô∏è –ò–ó–ú–ï–†–ï–ù–ò–ï: –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {process_time:.4f}—Å")
    
    all_words = []
    for r in results:
        all_words.extend([word for word, count in r['most_common']])
    
    overall_freq = Counter(all_words)
    
    tracer.log_var("total_sentences", len(sentences))
    tracer.log_var("total_tokens", total_tokens)
    tracer.log_var("total_filtered", total_filtered)
    
    tracer.log_instant("–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞]")
    
    total_time = time.time() - start_time
    tracer.log_instant(f"‚úÖ –ü–†–û–§–ò–õ–¨: –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω [{get_timestamp()}] - {total_time:.4f}—Å")
    
    return {
        'sentence_count': len(sentences),
        'total_tokens': total_tokens,
        'total_filtered': total_filtered,
        'overall_most_common': overall_freq.most_common(5)
    }

def main():
    start_time = time.time()
    tracer.log_instant(f"üöÄ –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´: [{get_timestamp()}]")
    
    tracer.log_instant("–ü—Ä–æ–≥—Ä–∞–º–º–∞[")
    
    tracer.log_var("start_time", time.strftime("%H:%M:%S"))
    
    result = analyze_text(sample_text)
    
    print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞:")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {result['sentence_count']}")
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {result['total_tokens']}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {result['total_filtered']}")
    print("–ù–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞:")
    for word, count in result['overall_most_common']:
        print(f"  - {word}: {count}")
    
    tracer.log_var("end_time", time.strftime("%H:%M:%S"))
    
    tracer.log_instant("–ü—Ä–æ–≥—Ä–∞–º–º–∞]")
    
    total_time = time.time() - start_time
    tracer.log_instant(f"üèÅ –ü–†–û–ì–†–ê–ú–ú–ê –ó–ê–í–ï–†–®–ï–ù–ê: [{get_timestamp()}] - –û–±—â–µ–µ –≤—Ä–µ–º—è {total_time:.4f}—Å")

if __name__ == "__main__":
    tracer = VizTracer(
        log_gc=True,
        max_stack_depth=10,
        output_file="viztracer_text_analysis.json"
    )
    
    tracer.start()
    
    main()
    
    tracer.stop()
    tracer.save()
